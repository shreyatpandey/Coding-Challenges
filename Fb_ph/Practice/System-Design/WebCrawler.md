# Introduction
  A web crawler is an Internet bot that systematically scours the world wide web (WWW) for content, starting its operation from a pool of seed URLs.
  This process of acquiring content from the WWW is called crawling. 
  It further saves the crawled content in the data stores. 
  The process of efficiently saving data for subsequent use is called storing.
  This is the first step that’s performed by search engines; the stored data is used for indexing and ranking purposes.
  This specific design problem is limited to web crawlers and doesn’t include explanations of the later stages of indexing and ranking in the search engines.
