# ğŸ Apple â€” Wi-Fi Software Development Engineer in Test: Full Interview Prep

> **Your Existing Prep:** RTT physics, FTM protocol, sensor fusion, Python automation script, C++ smart pointers.  
> **This Document:** Fills the remaining gaps â€” Apple-specific culture, system design, advanced coding, networking deep-dives, and behavioral prep.

---

## Part 1: Apple-Specific Context & Mindset

### Why This Role Exists at Apple
Apple's Wi-Fi team builds the **CoreWiFi / CoreLocation / Wi-Fi Positioning System (WPS)** stack used by *every* Apple device. As an SDET, you own the **quality of the entire wireless experience** â€” from the silicon (Wi-Fi chip) to the user-facing blue dot on Apple Maps.

### What Apple Expects From SDETs
| Trait | What It Means |
|-------|---------------|
| **Ownership** | You don't just write tests â€” you own a feature area end-to-end (test infra, CI/CD, metrics, release sign-off). |
| **Product Thinking** | "Will the user notice this 2 dB regression?" â€” Apple SDETs think in customer impact, not just pass/fail. |
| **Deep Technical** | You should be able to read the firmware engineer's packet captures, the driver engineer's logs, AND write the Python/C++ automation. |
| **Privacy First** | Apple's location stack is designed so the **device computes position locally**. The cloud never sees raw location. Expect questions about this philosophy. |

---

## Part 2: Wi-Fi & Networking Deep Dive

### 2.1 â€” 802.11 Standards Cheat Sheet
| Standard | Band | Max Rate | Key Feature | Test Relevance |
|----------|------|----------|-------------|----------------|
| 802.11n (Wi-Fi 4) | 2.4/5 GHz | 600 Mbps | MIMO, channel bonding | Legacy device compat |
| 802.11ac (Wi-Fi 5) | 5 GHz | 6.9 Gbps | MU-MIMO (downlink), 80/160 MHz | Throughput testing |
| 802.11ax (Wi-Fi 6/6E) | 2.4/5/6 GHz | 9.6 Gbps | OFDMA, TWT, BSS Coloring | Power, dense env testing |
| 802.11mc | 5 GHz | N/A | FTM / RTT for ranging | **Your core focus** |
| 802.11az | 2.4/5/6 GHz | N/A | Next-gen positioning (Secure LTF) | Future-proofing tests |
| 802.11be (Wi-Fi 7) | 2.4/5/6 GHz | 46 Gbps | MLO, 320 MHz, 4096-QAM | Bleeding-edge validation |

### 2.2 â€” Key Concepts You Must Know

**RSSI vs. RTT for Positioning:**
- **RSSI (signal strength):** Cheap, works everywhere, but accuracy is Â±5-15m due to fading/multipath.
- **RTT (round trip time):** Sub-meter accuracy, but requires 802.11mc/az support on the AP side.
- Apple likely uses a **hybrid** approach â€” RTT where available, RSSI + fingerprinting as fallback.

**Channel & Bandwidth:**
```
2.4 GHz: Channels 1-11 (US), 20/40 MHz BW
5 GHz:   Channels 36-165, 20/40/80/160 MHz BW
6 GHz:   Channels 1-233 (Wi-Fi 6E), up to 320 MHz BW (Wi-Fi 7)
```
- **Wider bandwidth = better RTT resolution** (timing resolution improves with BW).
- Test matrix must cover all bands Ã— bandwidths Ã— AP vendors.

**DFS (Dynamic Frequency Selection):**
- Channels 52-144 in 5 GHz require radar detection.
- Test: What happens to RTT session when a DFS channel switch occurs mid-measurement?

**Roaming (802.11r/k/v):**
- When a phone roams between APs, FTM sessions break. How does the test framework detect and re-establish ranging after a roam event?

### 2.3 â€” The TCP/IP Stack for SDETs
```
Application   â†’ HTTP, DNS, mDNS (Bonjour)
Transport     â†’ TCP (throughput), UDP (real-time)
Network       â†’ IP, ARP, DHCP, ICMPv6
Data Link     â†’ 802.11 MAC, WPA3, FTM frames
Physical      â†’ OFDM, OFDMA, beamforming
```
**Interview question:** "At which OSI layer does FTM operate?"  
**Answer:** Layer 2 (Data Link). FTM uses **Action Frames** (management frames), not IP packets. This means FTM works *without* IP connectivity â€” you don't need to be "connected" to an AP to range to it.

---

## Part 3: Test Strategy & Design

### 3.1 â€” RTT Test Matrix Design
**Q: "Design a test plan for validating RTT accuracy on the next iPhone."**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Test Pyramid for Wi-Fi RTT              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  â–²  Field Tests (5%)                                â”‚
â”‚  â”‚   - Real malls, airports, stadiums               â”‚
â”‚  â”‚   - Crowd density, real interference             â”‚
â”‚  â”‚                                                  â”‚
â”‚  â”‚  System Tests (20%)                              â”‚
â”‚  â”‚   - Full device, real APs, RF chamber            â”‚
â”‚  â”‚   - Multipath, NLOS, roaming scenarios           â”‚
â”‚  â”‚                                                  â”‚
â”‚  â”‚  Integration Tests (30%)                         â”‚
â”‚  â”‚   - WiFi stack + Location engine                 â”‚
â”‚  â”‚   - Mock AP (hostapd), controlled distances      â”‚
â”‚  â”‚                                                  â”‚
â”‚  â”‚  Unit Tests (45%)                                â”‚
â”‚  â”‚   - Algorithm validation (filter, trilateration) â”‚
â”‚  â”‚   - Timestamp math, edge cases                   â”‚
â”‚  â–¼   - Runs in CI, no hardware needed               â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 â€” Key Test Scenarios

| ID | Scenario | Setup | Pass Criteria |
|----|----------|-------|---------------|
| T1 | LOS accuracy | Open lab, 1-20m, 1m intervals | 90th percentile error < 1m |
| T2 | NLOS accuracy | Through 1 wall | Error < 3m (or correctly flagged NLOS) |
| T3 | Multi-AP trilateration | 3+ APs, known positions | Computed position error < 2m |
| T4 | Stability / Drift | Static device, 12h continuous | Std deviation < 0.5m |
| T5 | Bandwidth variation | 20/40/80/160 MHz | Accuracy improves with BW |
| T6 | Cross-vendor interop | Cisco, Aruba, Ubiquiti, Meraki | All pass T1 criteria |
| T7 | Power consumption | RTT ranging every 1s for 1h | Battery drain < X% |
| T8 | Coexistence | Bluetooth + RTT simultaneous | No degradation in RTT accuracy |
| T9 | Roaming | Move between 2 APs during ranging | Re-acquisition < 2s |
| T10 | DFS event | AP on DFS channel, radar detected | Graceful recovery |

### 3.3 â€” CI/CD for Hardware-in-the-Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Git Push â”‚ â”€â”€â–¶ â”‚  Jenkins /   â”‚ â”€â”€â–¶ â”‚ Test Farm    â”‚
â”‚  (Code)   â”‚     â”‚  CI Server   â”‚     â”‚ (Real HW)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚                     â”‚
                  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                  â”‚ Unit Tests â”‚         â”‚ Device Pool â”‚
                  â”‚ (no HW)   â”‚         â”‚ iPhone, APs â”‚
                  â”‚ < 5 min   â”‚         â”‚ RF Shield   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
- **Device management:** Serial/USB farm, each device flashed with the nightly build.
- **Result aggregation:** Grafana/InfluxDB dashboards tracking accuracy metrics over time.
- **Flaky test detection:** Auto-quarantine tests that fail intermittently (flag for infra vs. real issue).

---

## Part 4: Coding Questions (Python + C++)

### 4.1 â€” Python: RTT Trilateration
**Q: "Given 3 APs with known positions and measured distances, compute the device's 2D position."**

```python
import numpy as np

def trilaterate(ap_positions, distances):
    """
    Solve 2D trilateration using least squares.
    ap_positions: [(x1,y1), (x2,y2), (x3,y3)]
    distances:    [d1, d2, d3]
    
    Math: (x-xi)^2 + (y-yi)^2 = di^2
    Linearize by subtracting the last equation from all others.
    """
    n = len(ap_positions)
    # Use last AP as reference to linearize
    xn, yn = ap_positions[-1]
    dn = distances[-1]
    
    A = []
    b = []
    for i in range(n - 1):
        xi, yi = ap_positions[i]
        di = distances[i]
        # 2(xn - xi)*x + 2(yn - yi)*y = dn^2 - di^2 - xn^2 + xi^2 - yn^2 + yi^2
        A.append([2 * (xn - xi), 2 * (yn - yi)])
        b.append(dn**2 - di**2 - xn**2 + xi**2 - yn**2 + yi**2)
    
    A = np.array(A)
    b = np.array(b)
    
    # Least squares solution
    result, _, _, _ = np.linalg.lstsq(A, b, rcond=None)
    return result  # [x, y]

# Example: 3 APs in a room
aps = [(0, 0), (10, 0), (5, 10)]
dists = [5.0, 5.0, 5.0]
position = trilaterate(aps, dists)
print(f"Estimated position: ({position[0]:.2f}, {position[1]:.2f})")
```

### 4.2 â€” Python: Sliding Window Anomaly Detector
**Q: "In a continuous RTT stream, detect when measurements suddenly jump (potential AP crash or interference)."**

```python
from collections import deque
import statistics

class RTTAnomalyDetector:
    def __init__(self, window_size=20, threshold_sigma=3.0):
        self.window = deque(maxlen=window_size)
        self.threshold = threshold_sigma
    
    def feed(self, measurement):
        """Returns (is_anomaly, cleaned_value)"""
        if len(self.window) < 5:
            self.window.append(measurement)
            return False, measurement
        
        median = statistics.median(self.window)
        mad = statistics.median([abs(x - median) for x in self.window])
        # MAD-based threshold (robust to outliers, unlike std dev)
        mad_scaled = mad * 1.4826  # scale to approximate std dev
        
        if mad_scaled == 0:
            mad_scaled = 0.001  # avoid division by zero for constant data
            
        z_score = abs(measurement - median) / mad_scaled
        
        if z_score > self.threshold:
            return True, median  # anomaly â€” return median as clean value
        
        self.window.append(measurement)
        return False, measurement
```

### 4.3 â€” C++: Smart Pointers in a Test Framework Context
**Q: "Why would a Wi-Fi test framework use smart pointers? Show a real-world example."**

```cpp
#include <memory>
#include <vector>
#include <string>
#include <iostream>

// A test fixture managing expensive hardware resources
class AccessPointController {
public:
    AccessPointController(const std::string& ip) : ip_(ip) {
        std::cout << "Connected to AP at " << ip_ << "\n";
    }
    ~AccessPointController() {
        std::cout << "Released AP at " << ip_ << "\n";
    }
    
    // Non-copyable (you can't clone a physical AP connection)
    AccessPointController(const AccessPointController&) = delete;
    AccessPointController& operator=(const AccessPointController&) = delete;
    
    void configureFTMResponder(int channel) {
        std::cout << "AP " << ip_ << " configured as FTM responder on ch " << channel << "\n";
    }
    
private:
    std::string ip_;
};

class RTTTestSuite {
public:
    // unique_ptr: Test suite exclusively owns the AP connection
    void setUp() {
        ap_ = std::make_unique<AccessPointController>("192.168.1.1");
        ap_->configureFTMResponder(36);
    }
    
    // Even if test throws, unique_ptr guarantees AP is released
    void runTest() {
        // ... test logic that might throw ...
    }
    
    void tearDown() {
        ap_.reset();  // Explicitly release, or let destructor handle it
    }

private:
    std::unique_ptr<AccessPointController> ap_;
};

// shared_ptr: Multiple test cases share a calibration reference
class CalibrationData {
public:
    CalibrationData() { std::cout << "Loaded calibration data\n"; }
    ~CalibrationData() { std::cout << "Freed calibration data\n"; }
    double getExpectedDistance(int ap_id) { return 5.0; }  // placeholder
};

void multiTestSharing() {
    // Expensive to load â€” share across tests, freed when last test finishes
    auto calibration = std::make_shared<CalibrationData>();
    
    // Both test threads can safely hold a reference
    auto test1_cal = calibration;  // ref_count = 2
    auto test2_cal = calibration;  // ref_count = 3
    
    // calibration freed only when ALL references go out of scope
}
```

### 4.4 â€” Data Structures & Algorithms
These come up frequently in Apple coding rounds:

| Topic | Wi-Fi SDET Context |
|-------|--------------------|
| **Hash Maps** | MAC address â†’ AP metadata lookup |
| **Heaps / Priority Queues** | Select K nearest APs by RSSI for ranging |
| **Sliding Window** | Moving average filter for RTT stream |
| **Graphs (BFS/DFS)** | Network topology discovery, AP neighbor mapping |
| **Binary Search** | Find threshold value in sorted calibration table |
| **Sorting** | Rank APs by signal quality for FTM candidate selection |

**Example: K Nearest APs by RSSI**
```python
import heapq

def k_nearest_aps(scan_results, k):
    """
    scan_results: [{"bssid": "AA:BB:...", "rssi": -45, "ftm_capable": True}, ...]
    Return top-k FTM-capable APs by strongest RSSI.
    """
    ftm_aps = [ap for ap in scan_results if ap["ftm_capable"]]
    # Use a max-heap (negate RSSI since heapq is min-heap)
    return heapq.nlargest(k, ftm_aps, key=lambda ap: ap["rssi"])
```

---

## Part 5: System Design Questions

### 5.1 â€” "Design an Automated Wi-Fi RTT Test Lab"
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Test Orchestration Server                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Scheduler â”‚  â”‚ Fleet Mgrâ”‚  â”‚ Result DB â”‚  â”‚ Dashboard â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚       â”‚              â”‚              â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚              â”‚              â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
   â”‚           Test Execution Agent          â”‚
   â”‚  â€¢ Flash DUT with build                 â”‚
   â”‚  â€¢ Configure APs (channel, BW, FTM)     â”‚
   â”‚  â€¢ Run test scripts                     â”‚
   â”‚  â€¢ Collect logs, pcaps, metrics          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚               â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  DUT (iPhone)  â”‚   â”‚  AP Array       â”‚
   â”‚  on Robot Arm  â”‚   â”‚  Cisco/Aruba    â”‚
   â”‚  (for movement â”‚   â”‚  3+ units at    â”‚
   â”‚   simulation)  â”‚   â”‚  known coords   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²                     â–²
         â”‚     RF Shielded     â”‚
         â”‚       Chamber       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Design Decisions to Discuss:**
1. **Robot arm vs. manual placement** â€” Repeatability for distance sweeps.
2. **RF shielded room** â€” Eliminates external interference, makes tests deterministic.
3. **PDU (Power Distribution Unit)** â€” Remote power cycle APs when they hang.
4. **Ground truth system** â€” Laser rangefinder or surveyed floor markers.
5. **Log correlation** â€” Timestamp sync between DUT syslog, AP logs, and test script.

### 5.2 â€” "Design a Flaky Test Detection System"
Apple runs millions of tests. Flaky tests erode trust.

```python
# Simplified flaky test classifier
class FlakyTestDetector:
    def __init__(self, lookback_runs=50, flaky_threshold=0.1):
        self.lookback = lookback_runs
        self.threshold = flaky_threshold  # 10% failure rate = likely flaky
    
    def classify(self, test_id, recent_results):
        """
        recent_results: list of booleans (True=pass, False=fail)
        Returns: 'stable_pass', 'stable_fail', 'flaky', 'degrading'
        """
        results = recent_results[-self.lookback:]
        fail_rate = results.count(False) / len(results)
        
        if fail_rate == 0:
            return 'stable_pass'
        elif fail_rate > 0.9:
            return 'stable_fail'  # Real bug, not flaky
        elif fail_rate < self.threshold:
            return 'stable_pass'  # Rare failures, likely infra
        else:
            # Check trend: is it getting worse?
            first_half = results[:len(results)//2]
            second_half = results[len(results)//2:]
            if (second_half.count(False) / len(second_half)) > \
               (first_half.count(False) / len(first_half)) * 2:
                return 'degrading'  # Regression signal!
            return 'flaky'
```

---

## Part 6: Behavioral / Culture Fit (Apple Values)

### The STAR Method (Situation â†’ Task â†’ Action â†’ Result)
Apple behavioral interviews are rigorous. Prepare 5-6 stories covering:

### Key Themes & Sample Questions

**1. Attention to Detail / Craft**
> *"Tell me about a time you found a subtle bug that others missed."*

Prep: A story where your thoroughness caught an edge case â€” ideally RF/wireless related.

**2. Ambiguity & Ownership**
> *"Describe a project where the requirements were unclear. How did you move forward?"*

Prep: Show you **drove clarity** â€” wrote a test plan, proposed metrics, got stakeholder buy-in.

**3. Collaboration**
> *"Tell me about a time you disagreed with an engineer about whether something was a bug."*

Prep: Show you used **data** (logs, metrics, pcaps) to resolve the disagreement, not opinion.

**4. Impact & Scale**
> *"What's the most impactful test infrastructure you've built?"*

Prep: Quantify â€” "Reduced test cycle from 8h to 45m" or "Caught 3 P0 regressions before release."

**5. User Focus (Apple Specific)**
> *"How do you decide which tests to prioritize?"*

Prep: Frame it as **customer impact Ã— likelihood Ã— coverage gap**. Apple cares about user experience above all.

**6. Handling Failure**
> *"Tell me about a release where a bug escaped to production."*

Prep: Show root cause analysis, what you changed in the test process, and how you prevented recurrence.

### Things NOT to Say at Apple
- âŒ "It's not my job to debug the firmware."
- âŒ "We just need more test cases."
- âŒ "The test passed, so the feature works."
- âœ… "I investigated across the stack to find root cause."
- âœ… "I designed the test to catch this class of bugs."
- âœ… "The test gives us confidence, but here's what it doesn't cover..."

---

## Part 7: Tools & Technologies to Know

| Tool | Purpose | Your Talking Point |
|------|---------|-------------------|
| **Wireshark / tcpdump** | Packet capture & FTM frame analysis | "I use display filters like `wlan.fixed.action_code == 33` for FTM frames" |
| **hostapd** | Software AP / FTM responder | You already have this in your `wifi_rtt_test.py`! |
| **iw / iwconfig** | Wi-Fi interface control | Interface scanning, FTM initiation |
| **iperf3** | Throughput testing | Measure impact of FTM on data throughput |
| **XCTest / XCUITest** | Apple's native test framework | For on-device test automation |
| **Instruments** | Apple's profiler | Power/battery analysis during RTT |
| **CoreLocation API** | iOS location services | The API your tests validate |
| **Python (pytest)** | Test automation | Framework + fixtures + parametrize |
| **Git / CI (Jenkins/Xcode Cloud)** | Version control & automation | Nightly regression runs |
| **JIRA / Radar** | Bug tracking | Apple uses Radar internally |

---

## Part 8: Questions to Ask Your Interviewer

Strong questions signal genuine interest and technical depth:

1. **"How does the team handle the interop matrix? Do you maintain your own AP farm or work with vendor partners?"**
2. **"What's the current balance between real-hardware tests and simulation/emulation in the RTT pipeline?"**
3. **"How does the team measure and track location accuracy regressions across iOS releases?"**
4. **"What does the on-call rotation look like for the test infrastructure?"**
5. **"Is the team exploring 802.11az (Next Generation Positioning) yet, and how does that change the test strategy?"**
6. **"How does the team approach testing for privacy â€” e.g., ensuring the AP can't reverse-engineer the phone's position?"**

---

## Part 9: Day-Before Checklist

- [ ] Re-read your `wifi_rtt_test.py` â€” be ready to walk through it line by line.
- [ ] Re-derive the FTM distance formula: $d = c \times \frac{(t_4 - t_1) - (t_3 - t_2)}{2}$
- [ ] Practice explaining **multipath â†’ positive bias** in 30 seconds.
- [ ] Run through 2-3 LeetCode mediums (sliding window, heap, hash map).
- [ ] Prepare 5 STAR stories (written bullet points, not memorized scripts).
- [ ] Review Apple's latest Wi-Fi related announcements (WWDC, press releases).
- [ ] Get 8 hours of sleep. Seriously.

---

## Quick Reference Card (Print This)

```
FTM Distance = c Ã— [(t4-t1) - (t3-t2)] / 2
c = 3 Ã— 10^8 m/s
1 ns timing error â‰ˆ 0.15m distance error (each way)

RSSI: Signal strength, Â±5-15m accuracy
RTT:  Time-of-flight, sub-meter accuracy

802.11mc: Current RTT standard
802.11az: Next-gen, secure, better accuracy

FTM operates at Layer 2 (Action Frames)
No IP connectivity needed for ranging

Initiator = Phone (computes distance)
Responder = AP (returns timestamps, doesn't know distance)

Privacy: AP never learns phone's position.
```

---
*Good luck at Apple! ğŸ€ Remember: they're not just hiring a tester â€” they're hiring someone who cares deeply about the quality of every product that ships.*
